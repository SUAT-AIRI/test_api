# LLM Throughput Bench

A tiny, **OpenAIâ€‘compatible** CLI tool to benchmark Chat Completions API latency & throughput â€” supports **serial or concurrent** load, **streaming or nonâ€‘streaming** modes, accurate **perâ€‘request** metrics (TTFT / total latency / tok/s), and a **global throughput** number based on wallâ€‘clock time.

> Works with any endpoint that speaks the OpenAI Chat Completions protocol.

---

## âœ¨ Features

* **OpenAIâ€‘compatible**: point to any `v1/chat/completions` endpoint.
* **Streaming & nonâ€‘streaming**: measure TTFT precisely in streaming; fall back gracefully otherwise.
* **Concurrency**: simple threadâ€‘based load gen (`--concurrency N`).
* **Deterministic workloads**: reuse / shuffle questions, fix the seed.
* **CSV logs**: one row per request with all the details.
* **Global throughput**: total returned tokens Ã· wallâ€‘clock time.
* **No hard deps**: optional `tiktoken` for better token counting; otherwise a reasonable fallback.

---

## ğŸ“¦ Install

```bash
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

`requirements.txt`

```txt
requests>=2.31.0
tiktoken>=0.7.0  # optional
```

---

## ğŸ”§ Configure

Set environment variables for your endpoint:

```bash
export API_URL="http://<your-endpoint>/v1/chat/completions"
export API_KEY="sk-xxxxx"
# optional
export ORGANIZATION_ID="org_xxx"
```

> If your endpoint requires a custom header, adapt `build_headers()` in the script.

---

## â–¶ï¸ Run

### 1) Basic (serial, streaming)

```bash
python llm_throughput_bench.py --questions questions.txt
```

### 2) Concurrency (8 threads, 80 total requests)

```bash
python llm_throughput_bench.py \
  --questions questions.txt \
  --concurrency 8 \
  --total-requests 80
```

### 3) Nonâ€‘streaming

```bash
python llm_throughput_bench.py --questions questions.txt --no-stream
```

### 4) Shuffle & seed (deterministic)

```bash
python llm_throughput_bench.py --questions questions.txt --shuffle --seed 42
```

### 5) Throttle serial calls (light pacing)

```bash
python llm_throughput_bench.py --questions questions.txt --tiny-sleep 0.2
```

---

## ğŸ“š CLI Options

```text
--questions PATH          Question file (one per line). Default: questions.txt
--csv PATH                Output CSV filename. Default: results.csv
--model NAME              Model name. Default: DeepSeek-R1-bf16-hfd-w8a8
--max-tokens N            max_tokens per request. Default: 8192
--temperature FLOAT       Temperature. Default: 0.7
--timeout SECONDS         Per-request timeout. Default: 300
--concurrency N           Number of worker threads. Default: 1 (serial)
--total-requests N        Total number of requests (questions will be reused). Default: 10
--no-stream               Use non-streaming mode (default is streaming)
--shuffle                 Shuffle questions when building workload
--seed INT                RNG seed used with --shuffle. Default: 42
--tiny-sleep FLOAT        Serial mode only: sleep between requests (seconds)
```

---

## ğŸ“ questions.txt (example)

Put **one prompt per line**; blank lines are ignored. Example:

```txt
Explain the difference between precision and recall with a small numerical example.
ç»™æˆ‘ä¸€ä»½å…³äºTransformerä½ç½®ç¼–ç (PE/ALiBi/RoPE)çš„ç›´è§‚è§£é‡Šï¼Œæœ€å¥½é…å›¾æ€è·¯ã€‚
Write a Python function that computes the Levenshtein distance and show its complexity.
Summarize the main ideas behind variational autoencoders in 5 bullet points.
è®¾è®¡ä¸€ä¸ªå°å‹æ•°æ®åº“è¡¨ç»“æ„æ¥å­˜å‚¨è®ºæ–‡ã€ä½œè€…ã€æœºæ„ä¸å¼•ç”¨å…³ç³»ï¼Œå¹¶ç»™å‡º3æ¡ç¤ºä¾‹SQLæŸ¥è¯¢ã€‚
What are the trade-offs between Mixture-of-Experts and dense LLMs? Provide a table.
Provide a step-by-step guide to implement binary search on a rotated sorted array.
æ¯”è¾ƒRAGä¸Agentic RAGçš„æ¶æ„å·®å¼‚ã€é€‚ç”¨åœºæ™¯ä¸è¯„ä¼°æŒ‡æ ‡ã€‚
Why does label smoothing help generalization? Any caveats?
ç”¨é€šä¿—æ¯”å–»è§£é‡Šé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ä»¥åŠä»·å€¼è¿­ä»£çš„æ ¸å¿ƒæ€æƒ³ã€‚
```

> The tool will **cycle** through your questions when `--total-requests` exceeds the number of lines.

---

## ğŸ“Š Outputs

### Console summary

* **Success Rate**
* **Prompt tokens** (avg/p50/p95/min/max)
* **Response tokens** (avg/p50/p95/min/max)
* **TTFT** (Timeâ€‘Toâ€‘Firstâ€‘Token; streaming only, else `inf`)
* **Total latency** (request start â†’ done)
* **Perâ€‘request TPS** (response tokens Ã· total sec)
* **Global TPS** *(wallâ€‘clock throughput)*: $\texttt{sum(response tokens)} / \texttt{wall time}$

### CSV schema (`results.csv`)

| Column           | Meaning                                                |
| ---------------- | ------------------------------------------------------ |
| idx              | Request index (1â€‘based)                                |
| ok               | Boolean success flag                                   |
| prompt           | Original user prompt                                   |
| prompt\_tokens   | Token count of prompt                                  |
| ttft\_sec        | Time to first token (streaming; otherwise `inf`)       |
| total\_sec       | Endâ€‘toâ€‘end latency in seconds                          |
| response\_tokens | Token count of generated text                          |
| tokens\_per\_sec | Perâ€‘request throughput = response\_tokens / total\_sec |
| error            | Exception text, if any                                 |

> Token counting uses `tiktoken` if available (`cl100k_base`); otherwise a CJKâ€‘aware heuristic.

---

## ğŸ§ª Methodology & Metrics

* **TTFT**: Captured from the moment the HTTP request is sent until the first streamed content token is observed. Useful for *perceived responsiveness*.
* **Total latency**: Full request roundâ€‘trip time.
* **Perâ€‘request TPS**: Each requestâ€™s `response_tokens / total_sec`.
* **Global TPS (wallâ€‘clock)**: Sum of all successful response tokens divided by the **overall elapsed time** of the run. Reflects systemâ€‘level throughput with concurrency.

> In nonâ€‘streaming mode, TTFT is undefined and recorded as `inf`.

---

## ğŸ§° Tips for Fair Benchmarking

* Pin `--model`, `--max-tokens`, and `--temperature` across runs.
* Use the **same workload** (same `questions.txt`; optionally `--shuffle --seed` for randomization thatâ€™s reproducible).
* Warm up once before measuring.
* Prefer **streaming** when you care about perceived latency (TTFT).
* If testing rate limits, gradually increase `--concurrency` and watch **success rate** & errors.
* Keep an eye on **server logs** and GPU/CPU utilization to contextualize numbers.

---

## ğŸ›¡ï¸ Troubleshooting

**401 Unauthorized**

* Check `API_KEY`. Some backends require a prefix (`sk-`), some donâ€™t.

**404 or 405**

* Verify `API_URL` ends with `/v1/chat/completions` and the method is POST.

**429 Too Many Requests**

* Reduce `--concurrency`, add backoff (you can insert sleeps or a retry wrapper if your backend allows).

**Timeouts**

* Increase `--timeout` or reduce output lengths.

**Garbled streaming / JSONDecodeError**

* Some servers interleave heartbeats or comments; the script drops nonâ€‘JSON `data:` lines safely.

**Token counts look off**

* Install `tiktoken` for more accurate counts.

---

## ğŸ”’ Security Notes

* API keys are read from env vars and **never** written to disk.
* Avoid committing real keys in examples or CI.
* When filing issues, sanitize logs.

---

## ğŸ” Reproducibility

* The workload is deterministic with `--seed`.
* Concurrency uses threads; exact scheduling still depends on OS/runtime.
* Report: OS, Python, `requests`, `tiktoken` versions, and exact CLI.

---

## ğŸ¤ Compatibility

Tested against OpenAIâ€‘compatible servers. If your provider uses extra headers or different fields, tweak `build_headers()` or payload in `llm_throughput_bench.py`.

---

## ğŸ“„ License

MIT

---

## ğŸ™Œ Contributing

Issues and PRs are welcome! Please include:

* Endpoint type & version
* Full CLI
* A redacted snippet of `results.csv`
* Any serverâ€‘side logs/metrics you can share

---

## ğŸ“ Project Layout

```
.
â”œâ”€ llm_throughput_bench.py   # the CLI tool
â”œâ”€ requirements.txt          # requests (+ optional tiktoken)
â””â”€ questions.txt             # your prompts (one per line)
```

Happy benchmarking! ğŸš€
